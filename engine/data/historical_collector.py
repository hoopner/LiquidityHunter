"""
Historical Data Collector for LiquidityHunter
Collect and store 5 years of OHLCV data for 200 stocks
Optimized for M4 Max with parallel processing
"""
import asyncio
import os
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor

import yfinance as yf
import psycopg2
from psycopg2.extras import execute_batch


class HistoricalDataCollector:
    """
    Collect and store 5 years of historical OHLCV data
    Optimized for M4 Max with parallel processing
    """

    def __init__(self):
        # Database connection
        self.db_url = os.getenv('DATABASE_URL')
        if not self.db_url:
            raise ValueError("DATABASE_URL not set in environment")

        # Collection settings
        self.max_workers = 10   # Parallel downloads
        self.batch_size = 100   # Insert batch size
        self.years = 5          # Years of history

        # Stock lists
        self.kr_stocks = self._get_kr_stocks()
        self.us_stocks = self._get_us_stocks()

    def _get_kr_stocks(self) -> List[str]:
        """Top 100 Korean stocks"""
        # KOSPI top stocks
        kospi = [
            "005930.KS",  # ì‚¼ì„±ì „ìž
            "000660.KS",  # SKí•˜ì´ë‹‰ìŠ¤
            "035420.KS",  # NAVER
            "005380.KS",  # í˜„ëŒ€ì°¨
            "051910.KS",  # LGí™”í•™
            "035720.KS",  # ì¹´ì¹´ì˜¤
            "006400.KS",  # ì‚¼ì„±SDI
            "000270.KS",  # ê¸°ì•„
            "068270.KS",  # ì…€íŠ¸ë¦¬ì˜¨
            "105560.KS",  # KBê¸ˆìœµ
            "055550.KS",  # ì‹ í•œì§€ì£¼
            "028260.KS",  # ì‚¼ì„±ë¬¼ì‚°
            "012330.KS",  # í˜„ëŒ€ëª¨ë¹„ìŠ¤
            "017670.KS",  # SKí…”ë ˆì½¤
            "036570.KS",  # ì—”ì”¨ì†Œí”„íŠ¸
            "003550.KS",  # LG
            "032830.KS",  # ì‚¼ì„±ìƒëª…
            "009150.KS",  # ì‚¼ì„±ì „ê¸°
            "018260.KS",  # ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤
            "096770.KS",  # SKì´ë…¸ë² ì´ì…˜
            "034730.KS",  # SK
            "033780.KS",  # KT&G
            "010130.KS",  # ê³ ë ¤ì•„ì—°
            "086790.KS",  # í•˜ë‚˜ê¸ˆìœµì§€ì£¼
            "003490.KS",  # ëŒ€í•œí•­ê³µ
            "011200.KS",  # HMM
            "180640.KS",  # í•œì§„ì¹¼
            "047050.KS",  # í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„
            "010950.KS",  # S-Oil
            "047810.KS",  # í•œêµ­í•­ê³µìš°ì£¼
            "066570.KS",  # LGì „ìž
            "207940.KS",  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤
            "003670.KS",  # í¬ìŠ¤ì½”í™€ë”©ìŠ¤
            "015760.KS",  # í•œêµ­ì „ë ¥
            "030200.KS",  # KT
            "034220.KS",  # LGë””ìŠ¤í”Œë ˆì´
            "024110.KS",  # ê¸°ì—…ì€í–‰
            "316140.KS",  # ìš°ë¦¬ê¸ˆìœµì§€ì£¼
            "009540.KS",  # í•œêµ­ì¡°ì„ í•´ì–‘
            "010140.KS",  # ì‚¼ì„±ì¤‘ê³µì—…
            "329180.KS",  # í˜„ëŒ€ì¤‘ê³µì—…
            "000810.KS",  # ì‚¼ì„±í™”ìž¬
            "139480.KS",  # ì´ë§ˆíŠ¸
            "004020.KS",  # í˜„ëŒ€ì œì² 
            "000720.KS",  # í˜„ëŒ€ê±´ì„¤
            "005490.KS",  # í¬ìŠ¤ì½”í“¨ì²˜ì— 
            "352820.KS",  # í•˜ì´ë¸Œ
            "373220.KS",  # LGì—ë„ˆì§€ì†”ë£¨ì…˜
            "011070.KS",  # LGì´ë…¸í…
            "090430.KS",  # ì•„ëª¨ë ˆí¼ì‹œí”½
        ]

        # KOSDAQ top stocks
        kosdaq = [
            "247540.KQ",  # ì—ì½”í”„ë¡œë¹„ì— 
            "293490.KQ",  # ì¹´ì¹´ì˜¤ê²Œìž„ì¦ˆ
            "035900.KQ",  # JYP Ent.
            "086520.KQ",  # ì—ì½”í”„ë¡œ
            "067160.KQ",  # ì•„í”„ë¦¬ì¹´TV
            "091990.KQ",  # ì…€íŠ¸ë¦¬ì˜¨í—¬ìŠ¤ì¼€ì–´
            "263750.KQ",  # íŽ„ì–´ë¹„ìŠ¤
            "348370.KQ",  # ì•Œí…Œì˜¤ì  
            "066970.KQ",  # ì—˜ì•¤ì—í”„
            "145020.KQ",  # íœ´ì ¤
            "112040.KQ",  # ìœ„ë©”ì´ë“œ
            "041510.KQ",  # ì—ìŠ¤ì— 
            "357780.KQ",  # ì†”ë¸Œë ˆì¸
            "196170.KQ",  # ì•Œí…Œì˜¤ì  
            "131970.KQ",  # í…ŒìŠ¤ë‚˜
            "058470.KQ",  # ë¦¬ë…¸ê³µì—…
            "039030.KQ",  # ì´ì˜¤í…Œí¬ë‹‰ìŠ¤
            "214150.KQ",  # í´ëž˜ì‹œìŠ¤
            "068760.KQ",  # ì…€íŠ¸ë¦¬ì˜¨ì œì•½
            "095340.KQ",  # ISC
            "036930.KQ",  # ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§
            "222080.KQ",  # ì”¨ì•„ì´ì—ìŠ¤
            "141080.KQ",  # ë ˆê³ ì¼ë°”ì´ì˜¤
            "383220.KQ",  # F&F
            "028300.KQ",  # HLB
            "029960.KQ",  # ì½”ì—”í…
            "086900.KQ",  # ë©”ë””í†¡ìŠ¤
            "253450.KQ",  # ìŠ¤íŠœë””ì˜¤ë“œëž˜ê³¤
            "039200.KQ",  # ì˜¤ìŠ¤ì½”í…
            "078600.KQ",  # ëŒ€ì£¼ì „ìžìž¬ë£Œ
            "298380.KQ",  # ì—ì´ë¹„ì—˜ë°”ì´ì˜¤
            "122870.KQ",  # ì™€ì´ì§€ì—”í„°í…Œì¸ë¨¼íŠ¸
            "060310.KQ",  # 3S
            "323990.KQ",  # ë°•ì…€ë°”ì´ì˜¤
            "059090.KQ",  # ë¯¸ì½”
            "240810.KQ",  # ì›ìµIPS
            "108320.KQ",  # LXì„¸ë¯¸ì½˜
            "035760.KQ",  # CJ ENM
            "025980.KQ",  # ì•„ë‚œí‹°
            "194480.KQ",  # ë°ë¸Œì‹œìŠ¤í„°ì¦ˆ
            "096530.KQ",  # ì”¨ì  
            "038500.KQ",  # ì‚¼í‘œì‹œë©˜íŠ¸
            "101490.KQ",  # ì—ìŠ¤ì•¤ì—ìŠ¤í…
            "137310.KQ",  # ì—ìŠ¤ë””ë°”ì´ì˜¤ì„¼ì„œ
            "322510.KQ",  # ì œì´ì—˜ì¼€ì´
            "950140.KQ",  # ìž‰ê¸€ìš°ë“œëž©
            "277810.KQ",  # ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤
            "317530.KQ",  # ìºë¦¬ì†Œí”„íŠ¸
            "365340.KQ",  # ì„±ì¼í•˜ì´í…
            "060280.KQ",  # íë ‰ì†Œ
        ]

        return (kospi + kosdaq)[:100]  # Top 100

    def _get_us_stocks(self) -> List[str]:
        """Top 100 US stocks"""
        stocks = [
            # Mega caps
            "AAPL", "MSFT", "GOOGL", "AMZN", "NVDA", "META", "TSLA", "BRK-B",
            "V", "UNH", "JNJ", "WMT", "JPM", "MA", "PG", "XOM", "HD", "CVX",
            "MRK", "ABBV", "KO", "PEP", "COST", "AVGO", "TMO", "LLY", "NVO",
            "MCD", "CSCO", "ACN", "BAC", "CRM", "AMD", "NFLX", "ADBE", "VZ",

            # Growth tech
            "PLTR", "COIN", "RBLX", "SNOW", "NET", "DDOG", "CRWD", "ZS",
            "OKTA", "MDB", "SHOP", "SQ", "PYPL", "UBER", "LYFT", "ABNB",
            "DASH", "AFRM", "SOFI", "RIVN",

            # Traditional
            "DIS", "NKE", "INTC", "QCOM", "TXN", "INTU", "ISRG", "AMGN",
            "HON", "UNP", "LOW", "BA", "CAT", "DE", "MMM", "GE", "GM", "F",

            # Finance
            "GS", "MS", "C", "WFC", "AXP", "SPGI", "BLK", "SCHW",

            # Healthcare
            "PFE", "BMY", "GILD", "REGN", "VRTX", "BIIB", "MRNA", "CVS",

            # Energy
            "COP", "SLB", "EOG", "PXD",

            # Additional tech
            "ORCL", "IBM", "NOW", "PANW", "FTNT", "SPLK", "WDAY", "TEAM",
            "ZM", "DOCU", "TWLO", "TTD", "U", "ROKU",
        ]

        return stocks[:100]  # Top 100

    def download_symbol_history(self, symbol: str, market: str) -> Optional[List[Dict]]:
        """
        Download 5 years of daily data for a symbol
        Returns list of OHLCV dicts
        """
        try:
            print(f"  Downloading {symbol}...")

            # Calculate date range
            end_date = datetime.now()
            start_date = end_date - timedelta(days=365 * self.years)

            # Download data
            ticker = yf.Ticker(symbol)
            hist = ticker.history(
                start=start_date.strftime('%Y-%m-%d'),
                end=end_date.strftime('%Y-%m-%d'),
                interval='1d'
            )

            if hist.empty:
                print(f"    âš ï¸  {symbol}: No data")
                return None

            # Convert to list of dicts
            clean_symbol = symbol.replace('.KS', '').replace('.KQ', '')

            data = []
            for timestamp, row in hist.iterrows():
                data.append({
                    'timestamp': timestamp.to_pydatetime(),
                    'symbol': clean_symbol,
                    'market': market,
                    'open': float(row['Open']),
                    'high': float(row['High']),
                    'low': float(row['Low']),
                    'close': float(row['Close']),
                    'volume': int(row['Volume'])
                })

            print(f"    âœ“ {symbol}: {len(data)} records")
            return data

        except Exception as e:
            print(f"    âŒ {symbol}: {str(e)[:50]}")
            return None

    def store_data_batch(self, data: List[Dict]) -> int:
        """Store batch of OHLCV data in PostgreSQL"""
        if not data:
            return 0

        conn = psycopg2.connect(self.db_url)
        cur = conn.cursor()

        try:
            # Prepare data for batch insert
            records = [
                (
                    d['timestamp'],
                    d['symbol'],
                    d['market'],
                    d['open'],
                    d['high'],
                    d['low'],
                    d['close'],
                    d['volume']
                )
                for d in data
            ]

            # Batch insert with ON CONFLICT DO NOTHING (skip duplicates)
            execute_batch(
                cur,
                """
                INSERT INTO ohlcv_data
                (timestamp, symbol, market, open, high, low, close, volume)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING
                """,
                records,
                page_size=self.batch_size
            )

            conn.commit()
            return len(records)

        except Exception as e:
            conn.rollback()
            print(f"  Error storing batch: {e}")
            return 0

        finally:
            cur.close()
            conn.close()

    async def collect_all_data(self):
        """
        Main collection function
        Downloads all stocks in parallel and stores to DB
        """

        print("=" * 60)
        print("HISTORICAL DATA COLLECTION")
        print("=" * 60)
        print(f"Korean stocks: {len(self.kr_stocks)}")
        print(f"US stocks: {len(self.us_stocks)}")
        print(f"Years: {self.years}")
        print(f"Workers: {self.max_workers}")
        print("=" * 60)

        start_time = time.time()

        # Collect Korean stocks
        print("\nðŸ“Š Collecting Korean stocks...")
        kr_data = await self._collect_market(self.kr_stocks, "KR")

        # Collect US stocks
        print("\nðŸ“Š Collecting US stocks...")
        us_data = await self._collect_market(self.us_stocks, "US")

        # Statistics
        elapsed = time.time() - start_time
        total_records = kr_data + us_data

        print("\n" + "=" * 60)
        print("COLLECTION COMPLETE")
        print("=" * 60)
        print(f"Korean records: {kr_data:,}")
        print(f"US records: {us_data:,}")
        print(f"Total records: {total_records:,}")
        print(f"Time elapsed: {elapsed/60:.1f} minutes")
        if elapsed > 0:
            print(f"Records/second: {total_records/elapsed:.1f}")
        print("=" * 60)

        return total_records

    async def _collect_market(self, symbols: List[str], market: str) -> int:
        """Collect data for one market"""

        total_records = 0

        # Process in batches to avoid overwhelming system
        for i in range(0, len(symbols), self.max_workers):
            batch = symbols[i:i + self.max_workers]

            batch_num = i // self.max_workers + 1
            total_batches = (len(symbols) - 1) // self.max_workers + 1
            print(f"\nBatch {batch_num}/{total_batches}")

            # Download batch in parallel
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [
                    loop.run_in_executor(
                        executor,
                        self.download_symbol_history,
                        symbol,
                        market
                    )
                    for symbol in batch
                ]

                results = await asyncio.gather(*futures)

            # Store all data from this batch
            batch_records = 0
            for data in results:
                if data:
                    stored = self.store_data_batch(data)
                    batch_records += stored
                    total_records += stored

            print(f"  Batch stored: {batch_records:,} records")

        return total_records


# CLI entry point
if __name__ == "__main__":
    import dotenv
    dotenv.load_dotenv()

    collector = HistoricalDataCollector()
    asyncio.run(collector.collect_all_data())
